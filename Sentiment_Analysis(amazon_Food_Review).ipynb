{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis(amazon Food Review).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgUAIP57aSau"
      },
      "source": [
        "# Importing the libraries\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "# To count the iterations \n",
        "from tqdm import tqdm\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('Reviews.csv')\n",
        "\n",
        "# Dropping the dups in dataset\n",
        "dataset = dataset.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\n",
        "\n",
        "def removeHTMLTags(review):\n",
        "    soup = BeautifulSoup(review, 'lxml')\n",
        "    return soup.get_text()\n",
        "\n",
        "def removeApostrophe(review):\n",
        "    phrase = re.sub(r\"won't\", \"will not\", review)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", review)\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", review)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", review)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", review)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", review)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", review)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", review)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", review)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", review)\n",
        "    return phrase\n",
        "\n",
        "def removeAlphaNumericWords(review):\n",
        "     return re.sub(\"\\S*\\d\\S*\", \"\", review).strip()\n",
        "\n",
        "def removeSpecialChars(review):\n",
        "     return re.sub('[^a-zA-Z]', ' ', review)\n",
        "\n",
        "def scorePartition(x):\n",
        "    if x < 3:\n",
        "        return 0\n",
        "    return 1\n",
        "\n",
        "def doTextCleaning(review):\n",
        "    review = removeHTMLTags(review)\n",
        "    review = removeApostrophe(review)\n",
        "    review = removeAlphaNumericWords(review)\n",
        "    review = removeSpecialChars(review) \n",
        "    # Lower casing\n",
        "    review = review.lower()  \n",
        "    #Tokenization\n",
        "    review = review.split()\n",
        "    #Removing Stopwords and Lemmatization\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    review = [lmtzr.lemmatize(word, 'v') for word in review if not word in set(stopwords.words('english'))]\n",
        "    review = \" \".join(review)    \n",
        "    return review\n",
        "\n",
        "# Generalizing the score\n",
        "actualScore = dataset['Score']\n",
        "positiveNegative = actualScore.map(scorePartition) \n",
        "dataset['Score'] = positiveNegative\n",
        "\n",
        "# creating the document corpus\n",
        "corpus = []   \n",
        "for index, row in tqdm(dataset.iterrows()):\n",
        "    review = doTextCleaning(row['Text'])\n",
        "    corpus.append(review)\n",
        "\n",
        "# Creating the Bag of Words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#Creating a tranform\n",
        "cv = CountVectorizer(ngram_range=(1,3), max_features = 5000)\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "y = dataset.iloc[:,6].values\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.cross_validation import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
        "\n",
        "# Fitting Naive Bayes to the Training set\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Predict the sentiment for new review\n",
        "def predictNewReview():\n",
        "    newReview = input(\"Type the Review: \")\n",
        "\n",
        "    if newReview =='':\n",
        "        print('Invalid Review')  \n",
        "    else:\n",
        "        newReview = doTextCleaning(newReview)\n",
        "        new_review = cv.transform([newReview]).toarray()  \n",
        "        prediction =  classifier.predict(new_review)\n",
        "        print(prediction)\n",
        "        if prediction[0] == 1:\n",
        "            print( \"Positive Review\" )\n",
        "        else:        \n",
        "            print( \"Negative Review\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}